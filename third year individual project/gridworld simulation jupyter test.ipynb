{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| * | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\"\"\"This algorithm is built based on the structure of Jeremy Zhang's concise but intuitive gridworld implemnetation,\n",
    "   available at https://towardsdatascience.com/implement-grid-world-with-q-learning-51151747b455 ,\n",
    "   so credit goes to him.\n",
    "   This is a demonstration of Q learning agent obstacle avoidance in a small gridworld scenario,\n",
    "   assume deterministic action, agent uses epsilon-greedy action selection,\n",
    "   with a grid layout as such:\n",
    "   -----------------\n",
    "   0 | 1 | 2|  3|...\n",
    "   1 |\n",
    "   2 |\n",
    "   ...\n",
    "   each coordinate is expressed as (i,j), where i is row index, j is column index,\n",
    "   After the Q agent class, there is also an Expected Sarsa agent class implemented, performances are compared.\n",
    "\"\"\"\n",
    "#global variables\n",
    "GRID_ROWS=5                #different row and column numbers to break the symmetry, allows easy occurence of optimal path to goal\n",
    "GRID_COLS=6\n",
    "START=(GRID_ROWS-2,0)      #the starting position\n",
    "WIN=(0,GRID_COLS-1)        #the winning position that ends an epsiode\n",
    "\n",
    "def obs():\n",
    "    \"\"\"generates random number of obstacles in random (i,j) position,there is a maximum number of obstacles\n",
    "       (otherwise the agent might not be able to end episode and backpropagate the reward)\n",
    "       returns a list of tuples of obstacle position\n",
    "    \"\"\"\n",
    "    obs_list=[]\n",
    "    for num in range(np.random.choice([n for n in range(1,max(GRID_ROWS,GRID_COLS))])):\n",
    "        i=np.random.choice([n for n in range(GRID_ROWS)])\n",
    "        j=np.random.choice([n for n in range(GRID_COLS)])\n",
    "        if (i,j) not in obs_list:\n",
    "            if (i,j)!=START and (i,j)!=WIN:\n",
    "                obs_list.append((i,j))\n",
    "    return obs_list\n",
    "OBS_LIST=obs()\n",
    "\n",
    "\n",
    "\n",
    "#To simulate the agent-environment interation, two separate classes are defined\n",
    "class Env():                         #the environment class\n",
    "    def __init__(self,state=START):\n",
    "        self.grid=np.zeros([GRID_ROWS,GRID_COLS])     #setup the grid\n",
    "        self.obstacles=OBS_LIST\n",
    "        #self.obstacles=[(0,2),(2,2)]            #can overwrite manually\n",
    "        if len(self.obstacles)>0:\n",
    "            for tup in self.obstacles:\n",
    "                self.grid[tup]=-1\n",
    "        self.state=state\n",
    "        self.End=False\n",
    "    \n",
    "    \n",
    "    def env_step(self,action):\n",
    "        \"\"\"The environment decides the step,\n",
    "           argument is an action taken: could be one of \"up\",\"down\",\"left\",\"right\",\n",
    "           returns the next position on grid\n",
    "        \"\"\"\n",
    "        if action=='up':\n",
    "            possible_next_state=(self.state[0]-1,self.state[1])\n",
    "        elif action=='down':\n",
    "            possible_next_state=(self.state[0]+1,self.state[1])\n",
    "        elif action=='left':\n",
    "            possible_next_state=(self.state[0],self.state[1]-1)\n",
    "        elif action=='right':\n",
    "            possible_next_state=(self.state[0],self.state[1]+1)\n",
    "        else:\n",
    "            raise Exception(str(action)+'is not in available actions!')\n",
    "        #check if possible next state is legal\n",
    "        if possible_next_state[0]>=0 and possible_next_state[0]<GRID_ROWS:\n",
    "            if possible_next_state[1]>=0 and possible_next_state[1]<GRID_COLS:\n",
    "                if possible_next_state not in self.obstacles:\n",
    "                    next_state=possible_next_state\n",
    "                    return next_state\n",
    "        return self.state\n",
    "           \n",
    "    \n",
    "    def env_reward(self):\n",
    "        \"\"\"Only return reward of 100 if agent reaches target\n",
    "           all other moves receive 0 reward\n",
    "        \"\"\"\n",
    "        if self.state==WIN:\n",
    "            return 100\n",
    "        return 0\n",
    "    \n",
    "    def isitEnd(self):\n",
    "        if self.state==WIN:\n",
    "            self.End=True\n",
    "\n",
    "    def showgrid(self,onlygrid=False):\n",
    "        if not onlygrid:\n",
    "            self.grid[self.state]=1\n",
    "            self.grid[WIN]=2\n",
    "        for i in range(GRID_ROWS):\n",
    "            print('-'*4*GRID_COLS+'-')\n",
    "            gridrow='|'\n",
    "            for j in range(GRID_COLS):\n",
    "                if self.grid[i,j]==1:\n",
    "                    indicator=' *'\n",
    "                if self.grid[i,j]==-1:\n",
    "                    indicator=' X'\n",
    "                if self.grid[i,j]==0:\n",
    "                    indicator=' 0'\n",
    "                if self.grid[i,j]==2:\n",
    "                    indicator=' ^'\n",
    "                gridrow+=indicator+' |'\n",
    "            print(gridrow)\n",
    "        print('-'*(4*GRID_COLS+1))\n",
    "        \n",
    "Env().showgrid()         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent's current path:\n",
      "\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| * | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "up\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| * | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | * | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "up\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | * | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | * | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | * | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | * | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | * |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "down\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | * |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "up\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | * |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "up\n"
     ]
    }
   ],
   "source": [
    "#the Q agent class\n",
    "class Q_agent():\n",
    "    def __init__(self):\n",
    "        self.agent_Env=Env()\n",
    "        self.actions=['up','down','left','right']\n",
    "        self.agent_End=self.agent_Env.End\n",
    "        self.step_size=0.2\n",
    "        self.discount=0.9\n",
    "        self.epsilon=0.1\n",
    "        self.q_values={}         #action value q(s,a)\n",
    "        for i in range(GRID_ROWS):\n",
    "            for j in range(GRID_COLS):\n",
    "                self.q_values[(i,j)]={}\n",
    "                for action in self.actions:\n",
    "                    self.q_values[(i,j)][action]=0          #initialize q_values as a nested dictionary, state(in coordinate tuple) mapped to its actions, each action mapped to a q value\n",
    "        self.trace=[]            #a record of position and action pairs\n",
    "    \n",
    "    def choose_action(self):\n",
    "        \"\"\"the agent's behaviour policy is set to be epsilon greedy\n",
    "        \"\"\"\n",
    "        max_q_set=float('-inf')\n",
    "        if np.random.uniform(0,1)<self.epsilon:\n",
    "            action=np.random.choice(self.actions)           #agent takes exploratroy actions\n",
    "        else:\n",
    "            for act in self.actions:\n",
    "                current_position=self.agent_Env.state\n",
    "                q=self.q_values[current_position][act]\n",
    "                if q>max_q_set:\n",
    "                    max_q_set=q\n",
    "                    action=act                          #agent exploits by choosing the action that maximises q value\n",
    "        return action\n",
    "    \n",
    "    def take_action(self,action):\n",
    "        next_position=self.agent_Env.env_step(action)\n",
    "        updated_Env=Env(next_position)                  #update the environment with the action selected\n",
    "        return updated_Env\n",
    "    \n",
    "    def agent_steps(self,num_episodes):\n",
    "        \"\"\"Agent is on the move, it selects action and move steps according to the Env class,\n",
    "           once it has reached termnial state, it back-propagate reward to previous states,\n",
    "           thus updating their action values,\n",
    "           since steps other than the one leading to terminal state will receive zero reward,\n",
    "           the q values will not be updateds until the epsiode has ended.\n",
    "           The target policy is the maximum of all action values at the state.\n",
    "        \"\"\"\n",
    "        counter=0\n",
    "        while counter<num_episodes:\n",
    "            if not self.agent_Env.End:            #agent is trying to reach the goal\n",
    "                action=self.choose_action()\n",
    "                self.trace.append([self.agent_Env.state,action])  #record agent's trace of the current episode\n",
    "                #print(\"current position {0} action {1}\".format(self.agent_Env.state, action))\n",
    "                self.agent_Env=self.take_action(action)     #update both the agent and enviroment after one step\n",
    "                #print('next position:',self.agent_Env.state,'\\n')\n",
    "                self.agent_Env.isitEnd()\n",
    "                self.agent_End=self.agent_Env.End           #unify the epsiode status\n",
    "            else:       #agent has reached the goal, current epsisode has ended \n",
    "                q=self.agent_Env.env_reward()\n",
    "                for act in self.actions:\n",
    "                    self.q_values[self.agent_Env.state][act]=q     #no action value difference when at terminal state\n",
    "                for state,action in reversed(self.trace):   #to backpropagate the action value\n",
    "                    q=self.q_values[state][action]+self.step_size*(self.discount*q-self.q_values[state][action])  #use temporal difference\n",
    "                    self.q_values[state][action]=q     #update the q value\n",
    "                if counter<num_episodes-1:\n",
    "                    self.reset()    #reset the environment for the next game, but keep the final trace for visualisation\n",
    "                counter+=1\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.trace=[]\n",
    "        self.agent_Env=Env()\n",
    "        self.agent_End=self.agent_Env.End\n",
    "\n",
    "        \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    testq=Q_agent()\n",
    "    testq.agent_steps(50000)\n",
    "    #print('current Q values:',testq.q_values,'\\n')\n",
    "    print(\"agent's current path:\\n\")\n",
    "    for state,action in testq.trace:\n",
    "        Env(state).showgrid()\n",
    "        print(action)\n",
    "                        \n",
    "                \n",
    "        \n",
    "                \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent's current path:\n",
      "\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| * | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "up\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| * | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | * | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | * | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | * | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | * | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "up\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | * | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "right\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | X | ^ |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | * |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "| 0 | X | 0 | 0 | X | 0 |\n",
      "-------------------------\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "-------------------------\n",
      "up\n"
     ]
    }
   ],
   "source": [
    "#the Expected Sarsa agent class\n",
    "class Exp_Sarsa_agent():\n",
    "    def __init__(self):\n",
    "        self.agent_Env=Env()\n",
    "        self.actions=['up','down','left','right']\n",
    "        self.agent_End=self.agent_Env.End\n",
    "        self.step_size=0.2\n",
    "        self.discount=0.9\n",
    "        self.epsilon=0.1\n",
    "        self.q_values={}         #action value q(s,a)\n",
    "        for i in range(GRID_ROWS):\n",
    "            for j in range(GRID_COLS):\n",
    "                self.q_values[(i,j)]={}\n",
    "                for action in self.actions:\n",
    "                    self.q_values[(i,j)][action]=0          #initialize q_values as a nested dictionary, state(in coordinate tuple) mapped to its actions, each action mapped to a q value\n",
    "        self.trace=[]            #a record of position and action pairs\n",
    "    \n",
    "    def choose_action(self):\n",
    "        \"\"\"the agent's behaviour policy is set to be epsilon greedy\n",
    "        \"\"\"\n",
    "        max_q_set=float('-inf')\n",
    "        if np.random.uniform(0,1)<self.epsilon:\n",
    "            action=np.random.choice(self.actions)           #agent takes exploratroy actions\n",
    "        else:\n",
    "            for act in self.actions:\n",
    "                current_position=self.agent_Env.state\n",
    "                q=self.q_values[current_position][act]\n",
    "                if q>max_q_set:\n",
    "                    max_q_set=q\n",
    "                    action=act                          #agent exploits by choosing the action that maximises q value\n",
    "        return action\n",
    "    \n",
    "    def take_action(self,action):\n",
    "        next_position=self.agent_Env.env_step(action)\n",
    "        updated_Env=Env(next_position)                  #update the environment with the action selected\n",
    "        return updated_Env\n",
    "    \n",
    "    def agent_steps(self,num_episodes):\n",
    "        \"\"\"Agent is on the move, it selects action and move steps according to the Env class,\n",
    "           once it has reached termnial state, it back-propagate reward to previous states,\n",
    "           thus updating their action values,\n",
    "           since steps other than the one leading to terminal state will receive zero reward,\n",
    "           the q values will not be updateds until the epsiode has ended.\n",
    "           The target policy is the expected value of action values at the state.\n",
    "        \"\"\"\n",
    "        counter=0\n",
    "        while counter<num_episodes:\n",
    "            if not self.agent_Env.End:            #agent is trying to reach the goal\n",
    "                action=self.choose_action()\n",
    "                self.trace.append([self.agent_Env.state,action])  #record agent's trace of the current episode\n",
    "                #print(\"current position {0} action {1}\".format(self.agent_Env.state, action))\n",
    "                self.agent_Env=self.take_action(action)     #update both the agent and enviroment after one step\n",
    "                #print('next position:',self.agent_Env.state,'\\n')\n",
    "                self.agent_Env.isitEnd()\n",
    "                self.agent_End=self.agent_Env.End           #unify the epsiode status\n",
    "            else:       #agent has reached the goal, current epsisode has ended \n",
    "                q=self.agent_Env.env_reward()\n",
    "                for act in self.actions:\n",
    "                    self.q_values[self.agent_Env.state][act]=q     #no action value difference when at terminal state\n",
    "                for state,action in reversed(self.trace):   #to backpropagate the action value\n",
    "                    current_q=self.q_values[state]  \n",
    "                    non_q=0\n",
    "                    for idx,val in current_q.items():\n",
    "                        if not idx==action:             #non_q will be the expectation for all non-greedy action values\n",
    "                            non_q+=(self.epsilon/len(self.actions))*val       \n",
    "                    exp_q=(1-self.epsilon+self.epsilon/len(self.actions))*q+non_q   #combine with expection for greedy action value\n",
    "                    q=self.q_values[state][action]+self.step_size*(self.discount*exp_q-self.q_values[state][action])\n",
    "                    self.q_values[state][action]=q    #update the action value\n",
    "                if counter<num_episodes-1:\n",
    "                    self.reset()    #reset the environment for the next game, but keep the final trace for visualisation\n",
    "                counter+=1\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        self.trace=[]\n",
    "        self.agent_Env=Env()\n",
    "        self.agent_End=self.agent_Env.End\n",
    "\n",
    "        \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    teste=Exp_Sarsa_agent()\n",
    "    teste.agent_steps(50000)\n",
    "    #print('current action values:',teste.q_values,'\\n')\n",
    "    print(\"agent's current path:\\n\")\n",
    "    for state,action in teste.trace:\n",
    "        Env(state).showgrid()\n",
    "        print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over ten games Q agent uses 94 steps while Expected Sarsa agent uses 92 steps\n"
     ]
    }
   ],
   "source": [
    "q_trace=0\n",
    "e_trace=0\n",
    "for i in range(10):              #repeat for 10 times to reduce the effect of randomness \n",
    "    testq=Q_agent()\n",
    "    teste=Exp_Sarsa_agent()\n",
    "    testq.agent_steps(5000)\n",
    "    teste.agent_steps(5000)\n",
    "    q_trace+=len(testq.trace)\n",
    "    e_trace+=len(teste.trace)\n",
    "print('Over ten games Q agent uses {0} steps while Expected Sarsa agent uses {1} steps'.format(q_trace,e_trace))\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "#conclusion:\n",
    "#The difference between the two agents is minor over the selected number of games.\n",
    "#I believe the main reason is the reward setting of this particular environment.\n",
    "#As there is only one state has positive reward, rest are all 0 reward, no negative rewards,\n",
    "#the expectation and maximum action value for a given state would be close,\n",
    "#therefore Expected Sarsa does not distinguish from Q agent that much.\n",
    "#If the environment changes and hitting obstacle would have high negative reward,\n",
    "#then the expectation of action values would be much more affected,\n",
    "#I predict in that case with high negative rewards Expected Sarsa will really excel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
